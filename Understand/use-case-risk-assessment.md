---
id: risk-assessment
layout: loc
title: AI Use Case Risk Assessment
---

<style>
  input {
    margin-right: 0.5em; 
  }
  </style>

# AI Use Case Risk Assessment

This questionnaire is meant to assist staff in assessing the risk profile of an AI use case. The risk level will inform planning for the level of the risk mitigation efforts, estimated timeline for safety, quality and performance verification, and resources required.  

## What and why?

1.	Describe the AI or ML use case being proposed, include information about what function will be enhanced or automated through AI, what improvements are expected, or what specific capability would be added?      

2.	What is motivating your team to introduce AI or ML into this process? (Check all that apply)
  
    - [ ] Directive from leadership  
    - [ ] To achieve a strategic or operational goal currently out of reach    
    - [ ] High volume of existing backlog of work or cases  
    - [ ] Improve overall quality of output or service  
    - [ ] Lower time of completion for an existing process (time saved)  
    - [ ] The enhancement could perform tasks that humans could not accomplish in a reasonable period of time (scale)  
    - [ ] Use of innovative approaches, want to try something new.  
    - [ ] Opportunity to collaborate with internal or external partner.  
    - [ ] Other (please specify):      
    {:class="what"}

3.	Is the proposed enhancement in an area with an existing high degree of oversight, regulation, or litigation? 
    
    - [ ] A: Yes   
    - [ ] B: Could be  
    - [ ] C: I don’t know   
    - [ ] D: Probably Not  
    - [ ] E: No
    {:class="what"}

4.	How will the proposed AI capability or service ultimately function?
  
    - [ ] A. Fully automatically with no human review or oversight  
    - [ ] B: Partially automated with limited review of all outputs  
    - [ ] C: I don’t know  
    - [ ] D: Partially automated with regular review of a sample of outputs  
    - [ ] E: Partially automated with review of all outputs   
    - [ ] Other, describe:      
    {:class="what"}

5.	Human and AI tools both introduce error into workflows. What is the tolerance for error in this proposed use case? 
  
    - [ ] A: No tolerance                          
    - [ ] B: Low tolerance for error      
    - [ ] C: I don’t know   
    - [ ] D: Some tolerance  
    - [ ] E: High tolerance
    {:class="what"}

6.	Is the capability operational in other systems internally or externally? 
  
    - [ ] A: No    
    - [ ] B: Not that I know of    
    - [ ] C: I don’t know    
    - [ ] D: Yes, at a different organization    
    - [ ] E: Yes, at my organization     
    - [ ] F: If yes, where and what is it?      
    {:class="what"}

7.	What is the status of planning for the use case? 
    
    - [ ] A: Would be nice to have    
    - [ ] B: Is a strategic investment or improvement    
    - [ ] C: I don't know    
    - [ ] D: Affects a critical function    
    - [ ] E: Affects a critical function that only this organization performs
    {:class="what"}

8.	What would the consequence of not deploying the case or enhancement be?
    
    - [ ] A: Use case or enhancement could not be delivered at all  
    - [ ] B: Use case or enhancement would not be timely and/or costs would be too high  
    - [ ] C: I don’t know  
    - [ ] D: Use case or enhancement quality would not be as high  
    - [ ] E: Use case or enhancement would not improve or meet expectations  
    - [ ] F: Other, please specify
    {:class="what"}

9.	How do you currently assure high quality outcomes or do quality review for this capability (or related task)? 
  
    - [ ] A: None, outcomes from this workflow are not reviewed   
    - [ ] B: This is a new capability, so no review process exists yet  
    - [ ] C: I don’t know  
    - [ ] D: We have expert staff review   
    - [ ] E: We have expert staff review and there are written policies and/or standards for the capability  
    - [ ] F: Other, describe:     
    {:class="what"}

10.	What is the cost or impact of mistakes or errors in this use case? 
  
    - [ ] A: High – mistakes would result in catastrophic failures of critical workflows  
    - [ ] B: Moderately high – mistakes would be very costly to resolve and fix  
    - [ ] C: I don’t know   
    - [ ] D: Moderate – mistakes would cause some disruption   
    - [ ] E: Low – mistakes would have little impact
    {:class="what"}

11.	Are the impacts resulting from the AI capability in the use case reversible? 
  
    - [ ] A: Cannot reverse  
    - [ ] B: Difficult to reverse  
    - [ ] C: I don’t know   
    - [ ] D: Likely reversible  
    - [ ] E: Easily reversible 
    {:class="what"}

## Who?

12.	Who are the stakeholders to consult, keep informed, and gather input and feedback from on the proposed AI use case? 
13.	If successfully implemented, could this enhancement have major impacts on staff, either in terms of their numbers or their roles? 
  
    - [ ] A: Yes, many staff will be impacted  
    - [ ] B: Yes, some staff will be impacted   
    - [ ] C: I don’t know   
    - [ ] D: Yes, a few staff will be impacted   
    - [ ] E: No staff will be impacted
    {:class="who"}

14.	What is the current level of skill or experience relevant staff have in AI or ML? 
  
    - [ ] A: Complete beginner  
    - [ ] B: Novice  
    - [ ] C: I don’t know   
    - [ ] D: Moderately experienced  
    - [ ] E: Highly experienced
    {:class="who"}

15.	Will end users directly interact with the AI or ML enhancement?
  
    - [ ] A: Yes  
    - [ ] B: Yes, with staff support  
    - [ ] C: I don’t know   
    - [ ] D: Not directly, staff will mediate access  
    - [ ] E: No
    {:class="who"}

16.	Are end users or those impacted by the enhancement particularly vulnerable? 
  
      - [ ] A: Yes, end users are in multiple vulnerable groups  
      - [ ] B: Yes, end users are in at least one vulnerable group  
      - [ ] C: I don’t know   
      - [ ] D: End users are not vulnerable now but could be  
      - [ ] E: No
      {:class="who"}

17.	Will the proposed capability create or exacerbate barriers for persons with disabilities? 
  
    - [ ] A: Yes  
    - [ ] C: I don’t know   
    - [ ] D: No  
    - [ ] E: No, it will help break down barriers
    {:class="who"}

18.	Are there disclosure or permission-requesting requirements for using data that people are depicted in?
  
    - [ ] A: Yes  
    - [ ] C: I don’t know   
    - [ ] D: Not applicable  
    - [ ] E: No
    {:class="who"}

19.	Are people or groups who are potentially impacted by the proposed AI process represented in the training data set?
  
    - [ ] A: No  
    - [ ] C: I don’t know   
    - [ ] D: Not applicable  
    - [ ] E: Yes
    {:class="who"}

20.	Will the AI enhancement impact the rights, wellbeing or dignity of individuals?
  
    - [ ] A: Yes, negatively   
    - [ ] B: Yes, positively for some people   
    - [ ] C: I don’t know   
    - [ ] D: Yes, positively for all people   
    - [ ] E: No
    {:class="who"}

## Data

21.	Will the enhancement use data that contains personal information at any point in the automated system? 
  
    - [ ] A: Yes  
    - [ ] B: Yes, a limited amount  
    - [ ] C: I don’t know   
    - [ ] D: Yes, but it is removed  
    - [ ] E: No
    {:class="data"}

22.	Will the enhancement use copyrighted or otherwise restricted information at any point in the proposed AI system?           
  
    - [ ] A: Yes  
    - [ ] B: Yes, a limited amount  
    - [ ] C: I don’t know   
    - [ ] D: Yes, but we have permission  
    - [ ] E: No
    {:class="data"}

23.	Will your organization be supplying data to train, validate or fine tune the automated system? These data usually consist of machine-readable digital content with the accompanying metadata or labels at the granularity, specificity, or standard you want the model to recognize. For example, e-book files with their corresponding MARC catalog records would train a model to create MARC records from e-books. 
  
    - [ ] A: No, we will use a pre trained system    
    - [ ] B: No, we will need to create training or validation data  
    - [ ] C: I don’t know if we have training data for this use case  
    - [ ] D: Yes, but we still need to assemble it  
    - [ ] E: Yes, we have it identified and ready to transfer  
    - [ ] F: Other, describe:      
    {:class="data"}

24.	All data contains some type of bias or unbalance. Do you have a process to test or review datasets against biases or other unbalanced or unexpected outcomes? 
  
    - [ ] A: We are not aware of biases and do not review for imbalanced outcomes  
    - [ ] B: We are aware of biases and do not review for imbalanced outcomes  
    - [ ] C: I don’t know   
    - [ ] D: We are aware of biases and have expert staff review for imbalanced outcomes   
    - [ ] E: We are aware of biases and have expert staff and balanced datasets to benchmark against  
    - [ ] Other, describe:     
    {:class="data"}

## Assessment: 

Count and add up your marked answers in the chart below: 

|Section |	A |	B |	C |	D |	E	| F |
|-|-|-|-|-|-|-|
| What & Why	|||||||					
| Who?					|||||||	
| Data					|||||||	
| Subtotals					|||||||
{:width="100%" border="2px"}

**Mostly As** - Very high-risk use case. Plan for significant research, development and testing before implementation. Performance must meet the current state of the art for AI safety, trustworthiness, and quality. Quality benchmarks and controls need to be developed and thoroughly tested for even and consistent performance. 

**A lot of Bs** - High risk use case. Experiment, test and pilot these solutions prior to implementation. Baseline quality metrics need to be established. 

**Primarily Cs** - More information and detail are needed prior to moving forward.

**A lot of Fs** - Moderate risk use case. If you want to buy or acquire AI services for this risk level, verify published performance metrics are achievable with your data and use case. 

**Mostly Es** - Low risk use case.  It is likely appropriate to research and purchase tools or services available in the marketplace that meet quality standards.

A mix? Where do you have the most As and Bs? That will tell you where to focus risk mitigation efforts. Your Ds and Es will point to where the conditions for risk are lowering. 

These scores are not final nor are they exact. They are meant to inform a first level assessment of risk, feasibility, and prioritization for proposed AI use cases. The discussion and reflection this questionnaire facilitate are more significant than the score. There are a range of risks and costs involved in the implementation of AI. A higher risk level generally signifies higher costs and resources required for implementation.[^1].

[^1]: Risk categories are inspired by the FADGI evaluation parameters.
    Many of the questions are based on the Algorithmic Impact Assessment
    (Canada): <https://canada-ca.github.io/aia-eia-js/>

    